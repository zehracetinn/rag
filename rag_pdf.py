import numpy as np
import faiss
import requests
import torch
from pypdf import PdfReader
from sentence_transformers import SentenceTransformer


# --------------------------------------------------
# 1Ô∏è‚É£ PDF OKUMA
# --------------------------------------------------
def read_pdf(path):
    reader = PdfReader(path)
    text = ""
    for page in reader.pages:
        content = page.extract_text()
        if content:
            text += content + "\n"
    return text


# --------------------------------------------------
# 2Ô∏è‚É£ CHUNKING
# --------------------------------------------------
def chunk_text(text, chunk_size=800, overlap=150):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks


# --------------------------------------------------
# 3Ô∏è‚É£ LLaMA API
# --------------------------------------------------
def call_llama(prompt):
    url = "http://localhost:11434/api/generate"

    payload = {
        "model": "llama3-tr",
        "prompt": prompt,
        "stream": False
    }

    response = requests.post(url, json=payload)
    return response.json()["response"]


# --------------------------------------------------
# 4Ô∏è‚É£ MAIN
# --------------------------------------------------
if __name__ == "__main__":

    pdf_path = "ornek.pdf"

    text = read_pdf(pdf_path)
    chunks = chunk_text(text)

    print("Toplam chunk:", len(chunks))

    # --------------------------------------------------
    # üî• EMBEDDING MODEL (MPS VARSA KULLAN)
    # --------------------------------------------------
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    print("Embedding cihazƒ±:", device)

    embed_model = SentenceTransformer(
        "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        device=device
    )

    doc_embeddings = embed_model.encode(chunks)

    # --------------------------------------------------
    # üîç FAISS INDEX
    # --------------------------------------------------
    dimension = doc_embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(np.array(doc_embeddings))

    # --------------------------------------------------
    # ‚ùì SORU
    # --------------------------------------------------
    question = input("\nSorunuzu yazƒ±n: ").strip()

    query_embedding = embed_model.encode([question])

    # --------------------------------------------------
    # üî• HYBRID RETRIEVAL LOGIC
    # --------------------------------------------------
    summary_keywords = ["ana konu", "√∂zet", "genel", "tamamƒ±", "ne anlatƒ±yor"]

    if any(keyword in question.lower() for keyword in summary_keywords):
        print("\n‚ö° Summary Mode: T√ºm dok√ºman baƒülamƒ± kullanƒ±lƒ±yor.\n")
        context = "\n\n".join(chunks)
    else:
        print("\n‚ö° Semantic Search Mode: FAISS kullanƒ±lƒ±yor.\n")
        distances, indices = index.search(np.array(query_embedding), k=2)
        context = "\n\n".join([chunks[i] for i in indices[0]])

    print("\n--- SE√áƒ∞LEN BAƒûLAM ---\n")
    print(context)
    print("\n----------------------\n")

    # --------------------------------------------------
    # üß† G√ú√áL√ú RAG PROMPT
    # --------------------------------------------------
    prompt = f"""
Sen bir dok√ºman analiz sistemisin.

Kurallar:
- SADECE a≈üaƒüƒ±daki BAƒûLAM i√ßindeki bilgileri kullan.
- Baƒülam dƒ±≈üƒ±nda bilgi ekleme.
- Tahmin y√ºr√ºtme.
- Eƒüer cevap baƒülamda yoksa:
  "Bu bilgi dok√ºmanda bulunamadƒ±." yaz.
- Cevabƒ± kƒ±sa ve net ver.
- Cevabƒ± yalnƒ±zca T√ºrk√ße ver.

BAƒûLAM:
{context}

SORU:
{question}
"""

    answer = call_llama(prompt)

    print("\nCevap:\n")
    print(answer)
